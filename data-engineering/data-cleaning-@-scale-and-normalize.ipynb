{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning: scale & normalize\n",
    "\n",
    "This notebook is an abstraction of the Kaggle's 5-Day Challenge.\n",
    "\n",
    "The **goal** of this exercise is to clean inconsistent entries. \n",
    "\n",
    "The **evaluation** of the assignment will follow:\n",
    "\n",
    "* Design process and thinking as a data engineer.\n",
    "* Validation of knowledge on the different tools and steps throughout the process.\n",
    "* Storytelling and visualisation of the insights.\n",
    "\n",
    "Exercise **workflow**:\n",
    "\n",
    "* Import dependencies & download data from [here](https://www.kaggle.com/kemical/kickstarter-projects/download).\n",
    "* Scale vs. Normalization theory\n",
    "* Scale vs. Normalization assessment\n",
    "    \n",
    "Notes:\n",
    "\n",
    "* Write your code into the `TODO` cells\n",
    "* Feel free to choose how to present the results throughout the exercise, what libraries (e.g., seaborn, bokeh, etc.) and/or tools (e.g., PowerBI or Tableau)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5cd5061f-ae30-4837-a53b-690ffd5c5830",
    "_uuid": "9d82bf13584b8e682962fbb96131f2447d741679"
   },
   "source": [
    "## Preamble\n",
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "135a7804-b5f5-40aa-8657-4a15774e3666",
    "_uuid": "835cbe0834b935fb0fd40c75b9c39454836f4d5f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "________\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* Download the latest version of the dataset from [here](https://www.kaggle.com/kemical/kickstarter-projects/download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "62b9f021-5b80-43e2-bf60-8e0d5e22d572",
    "_uuid": "032a618abb98a28e60ab84376cf21402178f995d"
   },
   "source": [
    "## Scaling vs. Normalization theory\n",
    "____\n",
    "\n",
    "One of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that, in scaling, you're changing the *range* of your data while in normalization you're changing the *shape of the distribution* of your data. Let's talk a little more in-depth about each of these options. \n",
    "\n",
    "___\n",
    "\n",
    "### **Scaling**\n",
    "\n",
    "This means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1.  You want to scale data when you're using methods based on measures of how far apart data points, like [support vector machines, or SVM](https://en.wikipedia.org/wiki/Support_vector_machine) or [k-nearest neighbors, or KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). With these algorithms, a change of \"1\" in any numeric feature is given the same importance. \n",
    "\n",
    "For example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar! This clearly doesn't fit with our intuitions of the world. With currency, you can convert between currencies. But what about if you're looking at something like height and weight? It's not entirely clear how many pounds should equal one inch (or how many kilograms should equal one meter).\n",
    "\n",
    "By scaling your variables, you can help compare different variables on equal footing. To help solidify what scaling looks like, let's look at a made-up example. (Don't worry, we'll work with real data in just a second, this is just to help illustrate my point.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bfeb0419798d62d5cbeef78378b9de573d137f79"
   },
   "outputs": [],
   "source": [
    "sns.distplot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e0942c00-e306-4c64-a53a-e76d07cd937f",
    "_uuid": "e35280c753de7b963c4d812624c816c766ef4367"
   },
   "outputs": [],
   "source": [
    "# generate 1000 data points randomly drawn from an exponential distribution\n",
    "original_data = np.random.exponential(size = 1000)\n",
    "\n",
    "# mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns = [0])\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "# sns.distplot returns a histogram. Default: automatically chosen bins and Kernel Density estimation\n",
    "# with automatically determined parameters.\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed530656-2707-4978-835c-c665a9e25ec0",
    "_uuid": "a2523383e47af8d7902b75c5da7829b85553dcae"
   },
   "source": [
    "Notice that the *shape* of the data doesn't change, but that instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n",
    "\n",
    "___\n",
    "### Normalization\n",
    "\n",
    "Scaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "\n",
    "> **[Normal distribution:](https://en.wikipedia.org/wiki/Normal_distribution)** Also known as the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution is also known as the Gaussian distribution.\n",
    "\n",
    "In general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed. Some examples of these include t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes. (Pro tip: any method with \"Gaussian\" in the name probably assumes normality.)\n",
    "\n",
    "The method were  using to normalize here is called the [Box-Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). Let's take a quick peek at what normalizing some data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "851dc531-ea15-46f4-ba59-2e9be614856c",
    "_uuid": "e1484f70203b1a9335a557939398beb45b3a4fbd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(normalized_data[0], ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the *shape* of our data has changed. Before normalizing it was almost L-shaped. But after normalizing it looks more like the outline of a bell (hence \"bell curve\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "52011105-e1e3-4bb0-9b59-59614a96e3d4",
    "_uuid": "5975eb63a310ca983facc4a8b969e235fee58c74"
   },
   "source": [
    " \n",
    "\n",
    "___\n",
    "## Scaling vs. Normalization assessment\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* Assess whether scaling or normalization makes more sense. \n",
    "\n",
    "\n",
    "**Notes** \n",
    "\n",
    "* You want to build a linear regression model to predict someone's grades given how much time they spend on various activities during a normal school week.  You notice that your measurements for how much time students spend studying aren't normally distributed: some students spend almost no time studying and others study for four or more hours every day. Should you scale or normalize this variable?\n",
    "\n",
    "* You're still working on your grades study, but you want to include information on how students perform on several fitness tests as well. You have information on how many jumping jacks and push-ups each student can complete in a minute. However, you notice that students perform far more jumping jacks than push-ups: the average for the former is 40, and for the latter only 10. Should you scale or normalize these variables?\n",
    "\n",
    "**In linear regression model Y = X\\beta+e, the assumption is e \\sim N(0,\\sigma^2) rather than X \\sim N(0,\\sigma^2), so here we do not need to require the studying time is normally distributed. We should first plot the scatter plot of grade and studying time, if the plot fails to show us the linear relationship, then we might consider the transformation on grade and studying time. Another idea is we could first build the linear model and then check the residuals vs fitted values plot. If the spread of residuals becomes widely as fitted values increasing, we could do power transformation on the response variable.\n",
    "It is interesting to note that Z-transformation is regarded as a kind of scaling, not as normalization, by applying the above distinctions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc728697-ce3e-4890-b14d-597b2281f30d",
    "_uuid": "0c4d06413046e632dd1936095028587af3be0e47"
   },
   "source": [
    "### Scaling\n",
    "___\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* Scale the `usd_goal_real` from 0 to 1\n",
    "* Plot the original & scaled data together to compare\n",
    "* Scale the `goal` from 0 to 1\n",
    "* Plot the original & scaled data together to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc6457a4-0850-4ba8-b3d3-d13b0773a816",
    "_uuid": "027cd701d636d429da778526c55ceca1a90a3f7d"
   },
   "source": [
    "### Normalization\n",
    "___\n",
    "\n",
    "**TODO**\n",
    "\n",
    "* Normalize the `usd_pledged_real` column with a [`Box-Cox`](https://en.wikipedia.org/wiki/Power_transform#Box–Cox_transformation)\n",
    "* Plot the original & normalzied data together to compare\n",
    "* Normalize the `pledged` column with a [`Box-Cox`](https://en.wikipedia.org/wiki/Power_transform#Box–Cox_transformation)\n",
    "* Plot the original & normalzied data together to compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
